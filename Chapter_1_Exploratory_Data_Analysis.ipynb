{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 1 - Exploratory Data Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7ArNossbfOdN+9tbLELPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young-hwanlee/practical-statistics-for-data-scientists/blob/main/Chapter_1_Exploratory_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt20mgjCIqkc",
        "outputId": "fe2c4f50-d2d9-4e3d-b2dc-d285a92e0bf1"
      },
      "source": [
        "!git clone https://github.com/young-hwanlee/practical-statistics-for-data-scientists.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'practical-statistics-for-data-scientists'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed0kcRuTrNDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "90ac164a-ec94-44e8-b946-14e7f4525845"
      },
      "source": [
        "## Practical Statistics for Data Scientists (Python)\n",
        "## Chapter 1. Exploratory Data Analysis\n",
        "# > (c) 2019 Peter C. Bruce, Andrew Bruce, Peter Gedeck\n",
        "\n",
        "# Import required Python packages.\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import trim_mean\n",
        "from statsmodels import robust\n",
        "\n",
        "# !pip install wquantiles\n",
        "import wquantiles\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "try:\n",
        "    # !pip install common\n",
        "    import common\n",
        "    DATA = common.dataDirectory()\n",
        "except ImportError:\n",
        "    DATA = Path().resolve() / 'data'\n",
        "\n",
        "# Define paths to data sets. If you don't keep your data in the same directory as the code, adapt the path names.\n",
        "AIRLINE_STATS_CSV = DATA / 'airline_stats.csv'\n",
        "KC_TAX_CSV = DATA / 'kc_tax.csv.gz'\n",
        "LC_LOANS_CSV = DATA / 'lc_loans.csv'\n",
        "AIRPORT_DELAYS_CSV = DATA / 'dfw_airline.csv'\n",
        "SP500_DATA_CSV = DATA / 'sp500_data.csv.gz'\n",
        "SP500_SECTORS_CSV = DATA / 'sp500_sectors.csv'\n",
        "STATE_CSV = DATA / 'state.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: common in /usr/local/lib/python3.7/dist-packages (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-911a852e9f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install common'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'common' has no attribute 'dataDirectory'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "rxgyegaNHzRE",
        "outputId": "13e7b9e6-7a7c-4ddf-a281-a6a5f280826f"
      },
      "source": [
        "\n",
        "import common"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d62b200b5f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'common' has no attribute 'dataDirectory'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ8lIeqfGja_",
        "outputId": "e4f25a6e-18ac-46c2-f3e6-8dd8e8b8a673"
      },
      "source": [
        "DATA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/data')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "61_MAxJ_DNwO",
        "outputId": "5ffcaa96-97f7-4499-f72b-1f840f52a134"
      },
      "source": [
        "## Estimates of Location\n",
        "### Example: Location Estimates of Population and Murder Rates\n",
        "\n",
        "# Table 1-2\n",
        "state = pd.read_csv(STATE_CSV)\n",
        "print(state.head(8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d9b7e719f744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Table 1-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTATE_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/state.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ObAoXDpGV2q"
      },
      "source": [
        "# Compute the mean, trimmed mean, and median for Population. For `mean` and `median` we can use\n",
        "# the _pandas_ methods of the data frame. The trimmed mean requires the `trim_mean` function in _scipy.stats_.\n",
        "state = pd.read_csv(STATE_CSV)\n",
        "print(state['Population'].mean())\n",
        "\n",
        "print(trim_mean(state['Population'], 0.1))\n",
        "\n",
        "print(state['Population'].median())\n",
        "\n",
        "# Weighted mean is available with numpy. For weighted median, we can use the specialised\n",
        "# package `wquantiles` (https://pypi.org/project/wquantiles/).\n",
        "print(state['Murder.Rate'].mean())\n",
        "\n",
        "print(np.average(state['Murder.Rate'], weights=state['Population']))\n",
        "\n",
        "print(wquantiles.median(state['Murder.Rate'], weights=state['Population']))\n",
        "\n",
        "## Estimates of Variability\n",
        "\n",
        "# Table 1-2\n",
        "print(state.head(8))\n",
        "\n",
        "# Standard deviation\n",
        "\n",
        "print(state['Population'].std())\n",
        "\n",
        "# Interquartile range is calculated as the difference of the 75% and 25% quantile.\n",
        "\n",
        "print(state['Population'].quantile(0.75) - state['Population'].quantile(0.25))\n",
        "\n",
        "# Median absolute deviation from the median can be calculated with a method in _statsmodels_\n",
        "\n",
        "print(robust.scale.mad(state['Population']))\n",
        "print(abs(state['Population'] - state['Population'].median()).median() / 0.6744897501960817)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH77159GDO70"
      },
      "source": [
        "### Percentiles and Boxplots\n",
        "# _Pandas_ has the `quantile` method for data frames.\n",
        "\n",
        "print(state['Murder.Rate'].quantile([0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "\n",
        "# Table 1.4\n",
        "percentages = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
        "df = pd.DataFrame(state['Murder.Rate'].quantile(percentages))\n",
        "df.index = [f'{p * 100}%' for p in percentages]\n",
        "print(df.transpose())\n",
        "\n",
        "# _Pandas_ provides a number of basic exploratory plots; one of them are boxplots\n",
        "\n",
        "ax = (state['Population']/1_000_000).plot.box(figsize=(3, 4))\n",
        "ax.set_ylabel('Population (millions)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### Frequency Table and Histograms\n",
        "# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.\n",
        "\n",
        "binnedPopulation = pd.cut(state['Population'], 10)\n",
        "print(binnedPopulation.value_counts())\n",
        "\n",
        "# Table 1.5\n",
        "binnedPopulation.name = 'binnedPopulation'\n",
        "df = pd.concat([state, binnedPopulation], axis=1)\n",
        "df = df.sort_values(by='Population')\n",
        "\n",
        "groups = []\n",
        "for group, subset in df.groupby(by='binnedPopulation'):\n",
        "    groups.append({\n",
        "        'BinRange': group,\n",
        "        'Count': len(subset),\n",
        "        'States': ','.join(subset.Abbreviation)\n",
        "    })\n",
        "print(pd.DataFrame(groups))\n",
        "\n",
        "# _Pandas_ also supports histograms for exploratory data analysis.\n",
        "\n",
        "ax = (state['Population'] / 1_000_000).plot.hist(figsize=(4, 4))\n",
        "ax.set_xlabel('Population (millions)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### Density Estimates\n",
        "# Density is an alternative to histograms that can provide more insight into the distribution of the data points. Use the argument `bw_method` to control the smoothness of the density curve.\n",
        "\n",
        "ax = state['Murder.Rate'].plot.hist(density=True, xlim=[0, 12], \n",
        "                                    bins=range(1,12), figsize=(4, 4))\n",
        "state['Murder.Rate'].plot.density(ax=ax)\n",
        "ax.set_xlabel('Murder Rate (per 100,000)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Exploring Binary and Categorical Data\n",
        "\n",
        "# Table 1-6\n",
        "dfw = pd.read_csv(AIRPORT_DELAYS_CSV)\n",
        "print(100 * dfw / dfw.values.sum())\n",
        "\n",
        "# _Pandas_ also supports bar charts for displaying a single categorical variable.\n",
        "\n",
        "ax = dfw.transpose().plot.bar(figsize=(4, 4), legend=False)\n",
        "ax.set_xlabel('Cause of delay')\n",
        "ax.set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Correlation\n",
        "# First read the required datasets\n",
        "\n",
        "sp500_sym = pd.read_csv(SP500_SECTORS_CSV)\n",
        "sp500_px = pd.read_csv(SP500_DATA_CSV, index_col=0)\n",
        "\n",
        "# Table 1-7\n",
        "# Determine telecommunications symbols\n",
        "telecomSymbols = sp500_sym[sp500_sym['sector'] == 'telecommunications_services']['symbol']\n",
        "\n",
        "# Filter data for dates July 2012 through June 2015\n",
        "telecom = sp500_px.loc[sp500_px.index >= '2012-07-01', telecomSymbols]\n",
        "telecom.corr()\n",
        "print(telecom)\n",
        "\n",
        "# Next we focus on funds traded on major exchanges (sector == 'etf').\n",
        "\n",
        "etfs = sp500_px.loc[sp500_px.index > '2012-07-01', \n",
        "                    sp500_sym[sp500_sym['sector'] == 'etf']['symbol']]\n",
        "print(etfs.head())\n",
        "\n",
        "# Due to the large number of columns in this table, looking at the correlation matrix is cumbersome and it's more convenient to plot the correlation as a heatmap. The _seaborn_ package provides a convenient implementation for heatmaps.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "ax = sns.heatmap(etfs.corr(), vmin=-1, vmax=1, \n",
        "                 cmap=sns.diverging_palette(20, 220, as_cmap=True),\n",
        "                 ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# The above heatmap works when you have color. For the greyscale images, as used in the book, we need to visualize the direction as well. The following code shows the strength of the correlation using ellipses.\n",
        "\n",
        "from matplotlib.collections import EllipseCollection\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "def plot_corr_ellipses(data, figsize=None, **kwargs):\n",
        "    ''' https://stackoverflow.com/a/34558488 '''\n",
        "    M = np.array(data)\n",
        "    if not M.ndim == 2:\n",
        "        raise ValueError('data must be a 2D array')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw={'aspect':'equal'})\n",
        "    ax.set_xlim(-0.5, M.shape[1] - 0.5)\n",
        "    ax.set_ylim(-0.5, M.shape[0] - 0.5)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    # xy locations of each ellipse center\n",
        "    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n",
        "\n",
        "    # set the relative sizes of the major/minor axes according to the strength of\n",
        "    # the positive/negative correlation\n",
        "    w = np.ones_like(M).ravel() + 0.01\n",
        "    h = 1 - np.abs(M).ravel() - 0.01\n",
        "    a = 45 * np.sign(M).ravel()\n",
        "\n",
        "    ec = EllipseCollection(widths=w, heights=h, angles=a, units='x', offsets=xy,\n",
        "                           norm=Normalize(vmin=-1, vmax=1),\n",
        "                           transOffset=ax.transData, array=M.ravel(), **kwargs)\n",
        "    ax.add_collection(ec)\n",
        "\n",
        "    # if data is a DataFrame, use the row/column names as tick labels\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        ax.set_xticks(np.arange(M.shape[1]))\n",
        "        ax.set_xticklabels(data.columns, rotation=90)\n",
        "        ax.set_yticks(np.arange(M.shape[0]))\n",
        "        ax.set_yticklabels(data.index)\n",
        "\n",
        "    return ec\n",
        "\n",
        "m = plot_corr_ellipses(etfs.corr(), figsize=(5, 4), cmap='bwr_r')\n",
        "cb = fig.colorbar(m)\n",
        "cb.set_label('Correlation coefficient')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### Scatterplots\n",
        "# Simple scatterplots are supported by _pandas_. Specifying the marker as `$\\u25EF$` uses an open circle for each point.\n",
        "\n",
        "ax = telecom.plot.scatter(x='T', y='VZ', figsize=(4, 4), marker='$\\u25EF$')\n",
        "ax.set_xlabel('ATT (T)')\n",
        "ax.set_ylabel('Verizon (VZ)')\n",
        "ax.axhline(0, color='grey', lw=1)\n",
        "ax.axvline(0, color='grey', lw=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ax = telecom.plot.scatter(x='T', y='VZ', figsize=(4, 4), marker='$\\u25EF$', alpha=0.5)\n",
        "ax.set_xlabel('ATT (T)')\n",
        "ax.set_ylabel('Verizon (VZ)')\n",
        "ax.axhline(0, color='grey', lw=1)\n",
        "print(ax.axvline(0, color='grey', lw=1))\n",
        "\n",
        "## Exploring Two or More Variables\n",
        "# Load the kc_tax dataset and filter based on a variety of criteria\n",
        "\n",
        "kc_tax = pd.read_csv(KC_TAX_CSV)\n",
        "kc_tax0 = kc_tax.loc[(kc_tax.TaxAssessedValue < 750000) & \n",
        "                     (kc_tax.SqFtTotLiving > 100) &\n",
        "                     (kc_tax.SqFtTotLiving < 3500), :]\n",
        "print(kc_tax0.shape)\n",
        "\n",
        "### Hexagonal binning and Contours \n",
        "#### Plotting numeric versus numeric data\n",
        "\n",
        "# If the number of data points gets large, scatter plots will no longer be meaningful. Here methods that visualize densities are more useful. The `hexbin` method for _pandas_ data frames is one powerful approach.\n",
        "\n",
        "ax = kc_tax0.plot.hexbin(x='SqFtTotLiving', y='TaxAssessedValue',\n",
        "                         gridsize=30, sharex=False, figsize=(5, 4))\n",
        "ax.set_xlabel('Finished Square Feet')\n",
        "ax.set_ylabel('Tax Assessed Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# The _seaborn_ kdeplot is a two-dimensional extension of the density plot. The calculation of the 2D-density for the full dataset takes several minutes. It is sufficient to create the visualization with a smaller sample of the dataset. With 10,000 data points, creating the graph takes only seconds. While some details may be lost, the overall shape is preserved.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "sns.kdeplot(data=kc_tax0.sample(10000), x='SqFtTotLiving', y='TaxAssessedValue', ax=ax)\n",
        "ax.set_xlabel('Finished Square Feet')\n",
        "ax.set_ylabel('Tax Assessed Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### Two Categorical Variables\n",
        "# Load the `lc_loans` dataset\n",
        "\n",
        "lc_loans = pd.read_csv(LC_LOANS_CSV)\n",
        "\n",
        "# Table 1-8(1)\n",
        "crosstab = lc_loans.pivot_table(index='grade', columns='status', \n",
        "                                aggfunc=lambda x: len(x), margins=True)\n",
        "print(crosstab)\n",
        "\n",
        "# Table 1-8(2)\n",
        "df = crosstab.copy().loc['A':'G',:]\n",
        "df.loc[:,'Charged Off':'Late'] = df.loc[:,'Charged Off':'Late'].div(df['All'], axis=0)\n",
        "df['All'] = df['All'] / sum(df['All'])\n",
        "perc_crosstab = df\n",
        "print(perc_crosstab)\n",
        "\n",
        "### Categorical and Numeric Data\n",
        "# _Pandas_ boxplots of a column can be grouped by a different column.\n",
        "\n",
        "airline_stats = pd.read_csv(AIRLINE_STATS_CSV)\n",
        "airline_stats.head()\n",
        "ax = airline_stats.boxplot(by='airline', column='pct_carrier_delay',\n",
        "                           figsize=(5, 5))\n",
        "ax.set_xlabel('')\n",
        "ax.set_ylabel('Daily % of Delayed Flights')\n",
        "plt.suptitle('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# _Pandas_ also supports a variation of boxplots called _violinplot_.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.violinplot(data=airline_stats, x='airline', y='pct_carrier_delay',\n",
        "               ax=ax, inner='quartile', color='white')\n",
        "ax.set_xlabel('')\n",
        "ax.set_ylabel('Daily % of Delayed Flights')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### Visualizing Multiple Variables\n",
        "\n",
        "zip_codes = [98188, 98105, 98108, 98126]\n",
        "kc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes),:]\n",
        "kc_tax_zip\n",
        "\n",
        "def hexbin(x, y, color, **kwargs):\n",
        "    cmap = sns.light_palette(color, as_cmap=True)\n",
        "    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)\n",
        "\n",
        "g = sns.FacetGrid(kc_tax_zip, col='ZipCode', col_wrap=2)\n",
        "g.map(hexbin, 'SqFtTotLiving', 'TaxAssessedValue', \n",
        "      extent=[0, 3500, 0, 700000])\n",
        "g.set_axis_labels('Finished Square Feet', 'Tax Assessed Value')\n",
        "g.set_titles('Zip code {col_name:.0f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}